<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1000px;
}	
h1 {
    font-weight:300;
}

.disclaimerbox {
    background-color: #eee;		
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
}

video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
}

img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
}

img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
}

a:link,a:visited
{
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
  }

  td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
  }

  .vert-cent {
      position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

<html>
<head>
<title>Dual-Resolution Correspondence Networks</title>
<meta property="og:image" content="./asset/splash.png"/>
<meta property="og:title" content="Dual-Resolution Correspondence Networks. 2020." />
</head>

<body>
<br>
<center>
    <span style="font-size:42px">Dual-Resolution Correspondence Networks</span><br>

    <table align=center width=1000px>
        <tr>
            <td align=center width=1000px>
                <span style="font-size:22px"><a href="">Xinghui Li<sup>1</sup></a></span> &emsp;
                <span style="font-size:22px"><a href="http://www.hankai.org/">Kai Han<sup>2</sup></a></span> &emsp;
                <span style="font-size:22px"><a href="https://lishuda.wordpress.com/">Shuda Li<sup>1</sup></a></span> &emsp;
                <span style="font-size:22px"><a href="http://www.robots.ox.ac.uk/~victor/">Victor Prisacariu<sup>1</sup></a></span>
            </td>
        </tr>
    </table>


    <span style="font-size:21px"><sup>1</sup>Active Vision Lab & <sup>2</sup>Visual Geometry Group</span> <br>
    <span style="font-size:22px">Department of Engineering Science, University of Oxford</span>
    <br>
    <br>

    <table align=center width=900px>
        <tr>
            <td align=center width=300px>
                <span style="font-size:22px"><a href='./asset/DualRC_Net.pdf'> Paper [PDF]</a></span> &emsp;&emsp;
                <span style="font-size:22px"><a href='https://github.com/ActiveVisionLab/DualRC-Net'> Code [PyTorch]</a></span>

            </td>
        </tr>
    </table>
</center>
<br>

<table align=center width=800px>
    <tr>
        <td width=400px>
            <center>
                <img class="rounded" src = "./asset/splash.png" height="320px"></img>
                <br>
            </center>
        </td>
    </tr>
</table>

<br>
<hr>

<table align=center width=900px>
    <center><h1>Abstract</h1></center>
    <p>
    We tackle the problem of establishing dense pixel-wise correspondences between a pair of images. In this work, we introduce Dual-Resolution Correspondence Networks (DualRC-Net), to obtain pixel-wise correspondences in a coarse-to-fine manner. DualRC-Net extracts both coarse- and fine- resolution feature maps. The coarse maps are used to produce a full but coarse 4D correlation tensor, which is then refined by a learnable neighbourhood consensus module. The fine-resolution feature maps are used to obtain the final dense correspondences guided by the refined coarse 4D correlation tensor. The selected coarse-resolution matching scores allow the fine-resolution features to focus only on a limited number of possible matches with high confidence. In this way, DualRC-Net dramatically increases matching reliability and localisation accuracy, while avoiding to apply the expensive 4D convolution kernels on fine-resolution feature maps. We comprehensively evaluate our method on large-scale public benchmarks including HPatches, InLoc, and Aachen Day-Night. It achieves the state-of-the-art results on all of them.
    </p>
</table>
<hr>

<table align=center width=900px>
    <center><h1>BibTex</h1></center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
          @inproceedings{Li2020Dual,
            author    = {Xinghui Li and Kai Han and Shuda Li and Victor Prisacariu},
            title     = {Dual-Resolution Correspondence Networks},
            booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
            year      = {2020},
          }
</span>
        </pre>
</table>
<hr>

<table align=center width=1000px>
    <tr>
        <td width=400px>
            <left>
            <center><h1>Acknowledgments</h1></center>
            We gratefully acknowledge the support of the European Commission Project Multiple-actOrs Virtual EmpathicCARegiver for the Elder (MoveCare) and the EPSRC Programme Grant Seebibyte  EP/M013774/1.
            </left>
        </td>
    </tr>
</table>

<br><br>
<p style="text-align:center;font-size:16px;">
    Webpage template borrowed from <a href="https://richzhang.github.io/splitbrainauto/">Split-Brain Autoencoders, CVPR 2017</a>.
</p>
</body>
</html>

